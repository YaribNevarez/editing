% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{schmidhuber2015deep}
J.~Schmidhuber, ``Deep learning in neural networks: An overview,'' \emph{Neural
  networks}, vol.~61, pp. 85--117, 2015.

\bibitem{Taigman_2014_CVPR}
Y.~Taigman, M.~Yang, M.~Ranzato, and L.~Wolf, ``Deepface: Closing the gap to
  human-level performance in face verification,'' in \emph{Proceedings of the
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, June
  2014.

\bibitem{Design_Exploration_SbS_Trans20}
N.~Abderrahmane, E.~Lemaire, and B.~Miramond, ``Design space exploration of
  hardware spiking neurons for embedded artificial intelligence,'' \emph{Neural
  Networks}, vol. 121, pp. 366 -- 386, 2020.

\bibitem{Spinnaker_Trans13}
E.~{Painkras}, L.~A. {Plana}, J.~{Garside}, S.~{Temple}, F.~{Galluppi},
  C.~{Patterson}, D.~R. {Lester}, A.~D. {Brown}, and S.~B. {Furber},
  ``Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural
  network simulation,'' \emph{IEEE Journal of Solid-State Circuits}, vol.~48,
  no.~8, pp. 1943--1953, Aug 2013.

\bibitem{ernst2007efficient}
U.~Ernst, D.~Rotermund, and K.~Pawelzik, ``Efficient computation based on
  stochastic spikes,'' \emph{Neural computation}, vol.~19, no.~5, pp.
  1313--1343, 2007.

\bibitem{SNN_Survey_Trans19}
\BIBentryALTinterwordspacing
M.~Bouvier, A.~Valentian, T.~Mesquida, F.~Rummens, M.~Reyboz, E.~Vianello, and
  E.~Beigne, ``Spiking neural networks hardware implementations and challenges:
  A survey,'' \emph{J. Emerg. Technol. Comput. Syst.}, vol.~15, no.~2, Apr.
  2019. [Online]. Available: \url{https://doi.org/10.1145/3304103}
\BIBentrySTDinterwordspacing

\bibitem{mcdonnell2011benefits}
M.~D. McDonnell and L.~M. Ward, ``The benefits of noise in neural systems:
  bridging theory and experiment,'' \emph{Nature Reviews Neuroscience},
  vol.~12, no.~7, pp. 415--425, 2011.

\bibitem{Dapello2020.06.16.154542}
J.~Dapello, T.~Marques, M.~Schrimpf, F.~Geiger, D.~D. Cox, and J.~J. DiCarlo,
  ``Simulating a primary visual cortex at the front of cnns improves robustness
  to image perturbations,'' \emph{bioRxiv}, 2020.

\bibitem{davies2018loihi}
M.~Davies, N.~Srinivasa, T.-H. Lin, G.~Chinya, Y.~Cao, S.~H. Choday, G.~Dimou,
  P.~Joshi, N.~Imam, S.~Jain \emph{et~al.}, ``Loihi: A neuromorphic manycore
  processor with on-chip learning,'' \emph{IEEE Micro}, vol.~38, no.~1, pp.
  82--99, 2018.

\bibitem{TrueNorth_Trans15}
F.~{Akopyan}, J.~{Sawada}, A.~{Cassidy}, R.~{Alvarez-Icaza}, J.~{Arthur},
  P.~{Merolla}, N.~{Imam}, Y.~{Nakamura}, P.~{Datta}, G.~{Nam}, B.~{Taba},
  M.~{Beakes}, B.~{Brezzo}, J.~B. {Kuang}, R.~{Manohar}, W.~P. {Risk},
  B.~{Jackson}, and D.~S. {Modha}, ``Truenorth: Design and tool flow of a 65 mw
  1 million neuron programmable neurosynaptic chip,'' \emph{IEEE Trans. on
  Computer-Aided Design of Integrated Circuits and Systems}, vol.~34, no.~10,
  pp. 1537--1557, Oct 2015.

\bibitem{izhikevich2004model}
E.~M. Izhikevich, ``Which model to use for cortical spiking neurons?''
  \emph{IEEE transactions on neural networks}, vol.~15, no.~5, pp. 1063--1070,
  2004.

\bibitem{amunts2019human}
K.~Amunts, A.~C. Knoll, T.~Lippert, C.~M. Pennartz, P.~Ryvlin, A.~Destexhe,
  V.~K. Jirsa, E.~Dâ€™Angelo, and J.~G. Bjaalie, ``The human brain project --
  synergy between neuroscience, computing, informatics, and brain-inspired
  technologies,'' \emph{PLoS biology}, vol.~17, no.~7, p. e3000344, 2019.

\bibitem{rotermund2019Backpropagation}
D.~Rotermund and K.~R. Pawelzik, ``Back-propagation learning in deep
  spike-by-spike networks,'' \emph{Frontiers in Computational Neuroscience},
  vol.~13, p.~55, 2019.

\bibitem{nevarez2020accelerator}
Y.~Nevarez, A.~Garcia-Ortiz, D.~Rotermund, and K.~R. Pawelzik, ``Accelerator
  framework of spike-by-spike neural networks for inference and incremental
  learning in embedded systems,'' in \emph{2020 9th International Conference on
  Modern Circuits and Systems Technologies (MOCAST)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2020, pp. 1--5.

\bibitem{rotermund2018massively}
D.~Rotermund and K.~R. Pawelzik, ``Massively parallel {FPGA} hardware for
  spike-by-spike networks,'' \emph{bioRxiv}, 2019.

\bibitem{roy2019towards}
K.~Roy, A.~Jaiswal, and P.~Panda, ``Towards spike-based machine intelligence
  with neuromorphic computing,'' \emph{Nature}, vol. 575, no. 7784, pp.
  607--617, 2019.

\bibitem{bouvier2019spiking}
M.~Bouvier, A.~Valentian, T.~Mesquida, F.~Rummens, M.~Reyboz, E.~Vianello, and
  E.~Beigne, ``Spiking neural networks hardware implementations and challenges:
  A survey,'' \emph{ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, vol.~15, no.~2, pp. 1--35, 2019.

\bibitem{young2019review}
A.~R. Young, M.~E. Dean, J.~S. Plank, and G.~S. Rose, ``A review of spiking
  neuromorphic hardware communication systems,'' \emph{IEEE Access}, vol.~7,
  pp. 135\,606--135\,620, 2019.

\bibitem{rotermund2019recurrentsbs}
D.~Rotermund and K.~R. Pawelzik, ``Biologically plausible learning in a deep
  recurrent spiking network,'' \emph{bioRxiv}, 2019.

\bibitem{dayan2001theoretical}
P.~Dayan and L.~F. Abbott, \emph{Theoretical neuroscience: computational and
  mathematical modeling of neural systems}.\hskip 1em plus 0.5em minus
  0.4em\relax Computational Neuroscience Series, 2001.

\bibitem{zhang2018survey}
M.~ZHANG, G.~Zonghua, and P.~Gang, ``A survey of neuromorphic computing based
  on spiking neural networks,'' \emph{Chinese Journal of Electronics}, vol.~27,
  no.~4, pp. 667--674, 2018.

\bibitem{lotrivc2012applicability}
U.~Lotri{\v{c}} and P.~Buli{\'c}, ``Applicability of approximate multipliers in
  hardware neural networks,'' \emph{Neurocomputing}, vol.~96, pp. 57--65, 2012.

\bibitem{sarwar2016multiplier}
S.~S. Sarwar, S.~Venkataramani, A.~Raghunathan, and K.~Roy, ``Multiplier-less
  artificial neurons exploiting error resiliency for energy-efficient neural
  computing,'' in \emph{2016 Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp.
  145--150.

\bibitem{mrazek2016design}
V.~Mrazek, S.~S. Sarwar, L.~Sekanina, Z.~Vasicek, and K.~Roy, ``Design of
  power-efficient approximate multipliers for approximate artificial neural
  networks,'' in \emph{Proceedings of the 35th International Conference on
  Computer-Aided Design}, 2016, pp. 1--7.

\bibitem{du2014leveraging}
Z.~Du, K.~Palem, A.~Lingamneni, O.~Temam, Y.~Chen, and C.~Wu, ``Leveraging the
  error resilience of machine-learning applications for designing highly energy
  efficient accelerators,'' in \emph{2014 19th Asia and South Pacific design
  automation conference (ASP-DAC)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2014, pp. 201--206.

\bibitem{park2009dynamic}
J.~Park, J.~H. Choi, and K.~Roy, ``Dynamic bit-width adaptation in dct: An
  approach to trade off image quality and computation energy,'' \emph{IEEE
  transactions on very large scale integration (VLSI) systems}, vol.~18, no.~5,
  pp. 787--793, 2009.

\bibitem{han2013approximate}
J.~Han and M.~Orshansky, ``Approximate computing: An emerging paradigm for
  energy-efficient design,'' in \emph{2013 18th IEEE European Test Symposium
  (ETS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013, pp. 1--6.

\bibitem{gupta2011impact}
V.~Gupta, D.~Mohapatra, S.~P. Park, A.~Raghunathan, and K.~Roy, ``Impact:
  imprecise adders for low-power approximate computing,'' in \emph{IEEE/ACM
  International Symposium on Low Power Electronics and Design}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2011, pp. 409--414.

\bibitem{mittal2016survey}
S.~Mittal, ``A survey of techniques for approximate computing,'' \emph{ACM
  Computing Surveys (CSUR)}, vol.~48, no.~4, pp. 1--33, 2016.

\bibitem{venkataramani2015approximate}
S.~Venkataramani, S.~T. Chakradhar, K.~Roy, and A.~Raghunathan, ``Approximate
  computing and the quest for computing efficiency,'' in \emph{2015 52nd
  ACM/EDAC/IEEE Design Automation Conference (DAC)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2015, pp. 1--6.

\bibitem{courbariaux2015binaryconnect}
M.~Courbariaux, Y.~Bengio, and J.-P. David, ``Binaryconnect: Training deep
  neural networks with binary weights during propagations,'' in \emph{Advances
  in neural information processing systems}, 2015, pp. 3123--3131.

\bibitem{han2015deep}
S.~Han, H.~Mao, and W.~J. Dally, ``Deep compression: Compressing deep neural
  networks with pruning, trained quantization and huffman coding,'' \emph{arXiv
  preprint arXiv:1510.00149}, 2015.

\bibitem{hubara2017quantized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio, ``Quantized
  neural networks: Training neural networks with low precision weights and
  activations,'' \emph{The Journal of Machine Learning Research}, vol.~18,
  no.~1, pp. 6869--6898, 2017.

\bibitem{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi, ``Xnor-net: Imagenet
  classification using binary convolutional neural networks,'' in
  \emph{European conference on computer vision}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2016, pp. 525--542.

\bibitem{moons20160}
B.~Moons and M.~Verhelst, ``A 0.3--2.6 tops/w precision-scalable processor for
  real-time large-scale convnets,'' in \emph{2016 IEEE Symposium on VLSI
  Circuits (VLSI-Circuits)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016,
  pp. 1--2.

\bibitem{whatmough201714}
P.~N. Whatmough, S.~K. Lee, H.~Lee, S.~Rama, D.~Brooks, and G.-Y. Wei, ``14.3 a
  28nm soc with a 1.2 ghz 568nj/prediction sparse deep-neural-network engine
  with> 0.1 timing error rate tolerance for iot applications,'' in \emph{2017
  IEEE International Solid-State Circuits Conference (ISSCC)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2017, pp. 242--243.

\bibitem{sun2018xnor}
X.~Sun, S.~Yin, X.~Peng, R.~Liu, J.-s. Seo, and S.~Yu, ``Xnor-rram: A scalable
  and parallel resistive synaptic architecture for binary neural networks,'' in
  \emph{2018 Design, Automation \& Test in Europe Conference \& Exhibition
  (DATE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 1423--1428.

\bibitem{lecun1989optimal}
Y.~LeCun, J.~Denker, and S.~Solla, ``Optimal brain damage,'' \emph{Advances in
  neural information processing systems}, vol.~2, pp. 598--605, 1989.

\bibitem{hassibi1992second}
B.~Hassibi and D.~Stork, ``Second order derivatives for network pruning:
  Optimal brain surgeon,'' \emph{Advances in neural information processing
  systems}, vol.~5, pp. 164--171, 1992.

\bibitem{molchanov2016pruning}
P.~Molchanov, S.~Tyree, T.~Karras, T.~Aila, and J.~Kautz, ``Pruning
  convolutional neural networks for resource efficient inference,'' \emph{arXiv
  preprint arXiv:1611.06440}, 2016.

\bibitem{li2016pruning}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf, ``Pruning filters for
  efficient convnets,'' \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{liu2018rethinking}
Z.~Liu, M.~Sun, T.~Zhou, G.~Huang, and T.~Darrell, ``Rethinking the value of
  network pruning,'' \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem{rathi2018stdp}
N.~Rathi, P.~Panda, and K.~Roy, ``Stdp-based pruning of connections and weight
  quantization in spiking neural networks for energy-efficient recognition,''
  \emph{IEEE Transactions on Computer-Aided Design of Integrated Circuits and
  Systems}, vol.~38, no.~4, pp. 668--677, 2018.

\bibitem{sen2017approximate}
S.~Sen, S.~Venkataramani, and A.~Raghunathan, ``Approximate computing for
  spiking neural networks,'' in \emph{Design, Automation \& Test in Europe
  Conference \& Exhibition (DATE), 2017}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 193--198.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting,''
  \emph{The journal of machine learning research}, vol.~15, no.~1, pp.
  1929--1958, 2014.

\bibitem{wan2013regularization}
L.~Wan, M.~Zeiler, S.~Zhang, Y.~Le~Cun, and R.~Fergus, ``Regularization of
  neural networks using dropconnect,'' in \emph{International conference on
  machine learning}, 2013, pp. 1058--1066.

\bibitem{neftci2016stochastic}
E.~O. Neftci, B.~U. Pedroni, S.~Joshi, M.~Al-Shedivat, and G.~Cauwenberghs,
  ``Stochastic synapses enable efficient brain-inspired learning machines,''
  \emph{Frontiers in neuroscience}, vol.~10, p. 241, 2016.

\bibitem{srinivasan2016magnetic}
G.~Srinivasan, A.~Sengupta, and K.~Roy, ``Magnetic tunnel junction based
  long-term short-term stochastic synapse for a spiking neural network with
  on-chip stdp learning,'' \emph{Scientific reports}, vol.~6, p. 29545, 2016.

\bibitem{buesing2011neural}
L.~Buesing, J.~Bill, B.~Nessler, and W.~Maass, ``Neural dynamics as sampling: a
  model for stochastic computation in recurrent networks of spiking neurons,''
  \emph{PLoS Comput Biol}, vol.~7, no.~11, p. e1002211, 2011.

\bibitem{bellec2017deep}
G.~Bellec, D.~Kappel, W.~Maass, and R.~Legenstein, ``Deep rewiring: Training
  very sparse deep networks,'' \emph{arXiv preprint arXiv:1711.05136}, 2017.

\bibitem{chen20184096}
G.~K. Chen, R.~Kumar, H.~E. Sumbul, P.~C. Knag, and R.~K. Krishnamurthy, ``A
  4096-neuron 1m-synapse 3.8-pj/sop spiking neural network with on-chip stdp
  learning and sparse weights in 10-nm finfet cmos,'' \emph{IEEE Journal of
  Solid-State Circuits}, vol.~54, no.~4, pp. 992--1002, 2018.

\bibitem{sheik2016synaptic}
S.~Sheik, S.~Paul, C.~Augustine, C.~Kothapalli, M.~M. Khellah, G.~Cauwenberghs,
  and E.~Neftci, ``Synaptic sampling in hardware spiking neural networks,'' in
  \emph{2016 IEEE International Symposium on Circuits and Systems
  (ISCAS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 2090--2093.

\bibitem{jerry2017ultra}
M.~Jerry, A.~Parihar, B.~Grisafe, A.~Raychowdhury, and S.~Datta, ``Ultra-low
  power probabilistic imt neurons for stochastic sampling machines,'' in
  \emph{2017 Symposium on VLSI Circuits}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. T186--T187.

\bibitem{kim2013energy}
Y.~Kim, Y.~Zhang, and P.~Li, ``An energy efficient approximate adder with carry
  skip for error resilient neuromorphic vlsi systems,'' in \emph{2013 IEEE/ACM
  International Conference on Computer-Aided Design (ICCAD)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2013, pp. 130--137.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein \emph{et~al.}, ``Imagenet large scale
  visual recognition challenge,'' \emph{International journal of computer
  vision}, vol. 115, no.~3, pp. 211--252, 2015.

\bibitem{lecun1998mnist}
Y.~LeCun, ``The mnist database of handwritten digits,'' \emph{http://yann.
  lecun. com/exdb/mnist/}, 1998.

\bibitem{xilinx2015zynq}
U.~Xilinx, ``Zynq-7000 all programmable soc: Technical reference manual,''
  2015.

\bibitem{hrica2012floating}
J.~Hrica, ``Floating-point design with vivado hls,'' \emph{Xilinx Application
  Note}, 2012.

\end{thebibliography}
