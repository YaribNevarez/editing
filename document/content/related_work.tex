\section{Related work}
\label{sec:related_work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximate computing in neural networks}
Approximate computing has been used in a wide range of applications to increase the computational efficiency in hardware\cite{han2013approximate}. For neural network applications, two main approximation strategies are used, namely network compression and classical approximate computing\cite{bouvier2019spiking}.

\subsubsection{Network compression}
Researchers focusing on embedded applications started lowering the precision of weight and activation map to shrink the memory footprint of the large number of parameters representing ANNs, a method known as network compression or quantization. Taking advantage of the intrinsic error-tolerance of neural networks, as well as their ability to compensate for approximation while training, reduced bit precision causes a small accuracy loss \cite{courbariaux2015binaryconnect, han2015deep, hubara2017quantized, rastegari2016xnor}.

In hardware deployment, weight quantization (WQ) has shown up to $2\times$ improvement in energy consumption with an accuracy degradation of less than $1\%$ \cite{moons20160, whatmough201714}. Some advanced quantization methods yield to binary neural networks (BNNs) allowing the use of XNORs instead of the conventional costly MACs \cite{rastegari2016xnor}. In \cite{sun2018xnor}, Sun et al. report an accuracy of $98.43\%$ on MNIST with a simple BNN. Hence, quantization is a powerful tool for improving the energy efficiency and memory requirements of ANN accelerators, with limited accuracy degradation.

These methods can be used for SNNs as well. In \cite{rathi2018stdp}, Rathi et al. report up to $3.1\times$ improvement in energy consumption with an accuracy loss of around $3\%$. WQ allows the designer to realize a tradeoff between the accuracy of the SNN application against energy and area utilization of the neural networks. Approximate computing can also be applied at the neuron level, where irrelevant units are deactivated to reduce the computation cost of the SNNs \cite{sen2017approximate}. This computation skipping can be applied randomly on synapses, training ANNs with stochastic synapses improves generalization, resulting in a better accuracy\cite{srivastava2014dropout, wan2013regularization}. Such method is compatible with SNNs and has been tested both during training \cite{neftci2016stochastic, srinivasan2016magnetic} and operation \cite{buesing2011neural}, and even to define the connectivity between layers \cite{bellec2017deep, chen20184096}. Implementations of spiking neuromorphic systems in FPGA \cite{sheik2016synaptic} and hardware \cite{jerry2017ultra} demonstrated that synaptic stochasticity allows to increase the final accuracy of the networks while reducing memory footprint.

Quantization is therefore a powerful technique to improve energy efficiency and memory requirements of ANN and SNN accelerators, with small accuracy degradation; however, this approach requires quantization-aware training methods that, in some cases, are problematic or even inaccessible, particularly in emerging deep SNN algorithms\cite{zhang2018survey}.

\subsubsection{Classical approximate computing}
This approach consists of designing processing elements that approximate their computation by employing modified algorithmic logic units \cite{han2013approximate}. In \cite{kim2013energy}, Kim et al. have shown SNNs using carry skip adders achieving $2.4\times$ latency enhancement and $43\%$ more energy efficiency, with an accuracy degradation of 0.97\% on MNIST classification task. Therefore, approximate computing provides important enhancement in energy efficiency and processing speed.

However, as the complexity of the dataset increases, as well as the depth of the network topology, such as ResNet \cite{he2016deep} on ImageNet \cite{russakovsky2015imagenet}, the accuracy degradation becomes more important and may not be negligible anymore \cite{rastegari2016xnor}, especially for critical applications such as autonomous driving. Therefore, it is not certain that network compression techniques and approximate computing are suitable for any application.

\subsection{Spike-by-Spike neural networks accelerators}
In an earlier research, Rotermund et al. demonstrated the feasibility of a neuromorphic SbS IP on a Xilinx Virtex 6 FPGA \cite{rotermund2018massively}. It provides a massively parallel architecture, optimized for memory access and suitable for ASIC implementations. Nonetheless, this design is considerably resource-demanding to be implemented as a complete SbS network in today's embedded technology. In \cite{nevarez2020accelerator}, we presented a cross-platform accelerator framework for design exploration and testing of fully functional SbS network models in embedded systems. As a hardware/software (HW/SW) co-design solution, this framework offers a comprehensive high level software API that allows the construction of scalable sequential SbS networks with configurable hardware acceleration. We use this design exploration framework to investigate approximate computing for efficient deployment of deep SbS networks on resource-limited devices.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%