\section{Related work}
\label{sec:related_work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spiking neural networks}
Recently, some state of the art hardware architecture surveys have been reported for SNN \cite{Design_Exploration_SbS_Trans20, SNN_Survey_Trans19, zhang2018survey}. Nassim Abderrahmane et al. briefly describe and compare some recent ASIC and FPGA implementations where only two are suitable for embedded systems. In \cite{Spinnaker_Trans13}, Furber et al. present SpiNNaker, which is suitable for neuroscience research but not for embedded applications. Further on, in earlier research Rotermund et al. demonstrated the feasibility of a neuromorphic SbS IP on a Xilinx Virtex 6 FPGA \cite{rotermund2018massively}. It provides a massively parallel architecture, optimized for memory access and suitable for ASIC implementations. However, this design is considerably resource-demanding to be implemented as a complete SbS network in today's embedded technology.

In \cite{nevarez2020accelerator}, we presented a cross-platform accelerator framework for design exploration and testing of fully functional SbS network models in embedded systems. As a hardware/software (HW/SW) co-design solution, this framework offers a comprehensive high level software API that allows the construction of scalable sequential SbS networks with configurable hardware acceleration.

\subsection{Approximate computing in Spiking neural networks}
Approximate computing has been used in a wide range of applications to increase the computational efficiency in hardware \cite{han2013approximate}. For neural network applications, two main approximation strategies are used, namely network compression and classical approximate computing.
Due to the large amount of parameters representing the neural networks, researchers targeting embedded applications started to reduce the weight and activation map precision to decrease the memory footprint of ANNs, a method referred to as network compression or quantization.
Thanks to the fault tolerance of neural networks, as well as their ability to compensate for approximation while training, reduced bit precision entails only a small loss in accuracy \cite{courbariaux2015binaryconnect, han2015deep, hubara2017quantized, rastegari2016xnor}.
Once transposed to hardware, weight quantization (WQ) has shown 1.5 to 2 times gains in energy
with an accuracy loss of less than 1\% \cite{moons20160, whatmough201714}. An aggressive quantization to binary neural networks (BNNs) allows to use XNORs instead of the traditional costly MACs \cite{rastegari2016xnor}. An interesting implementation leveraging both the parallel design of the crossbar array and the XNOR-net implementation is realized in Sun et al. \cite{sun2018xnor}. They report 98.43\% accuracy on MNIST with a simple BNN. Quantization is thus a powerful tool to improve energy efficiency and memory requirements of ANN accelerators, with limited accuracy loss.

These methods can be used for SNNs as well \cite{jin2017performance, rathi2018stdp}. Indeed, Rathi et al. \cite{rathi2018stdp} report from 2.2 to 3.1 times gain in energy, and even more in area, for an accuracy loss of around 3\%. What is interesting with WQ is that the designer can realize a tradeoff between the accuracy of the SNN application against energy and area requirements of the neural networks. Approximate computing can also be achieved at the neuron level, where insignificant units are deactivated to reduce the computation cost of evaluating SNNs \cite{sen2017approximate}.
Moreover, such computation skipping can be implemented at the synapse level in a random manner. Indeed, training ANNs with stochastic synapses enables a better generalization, which results in a better accuracy on the test sets \cite{srivastava2014dropout, wan2013regularization}. Again, this method is compatible with SNNs and has been tested both during training \cite{neftci2016stochastic, srinivasan2016magnetic} and operation \cite{buesing2011neural}, and even to define the connectivity between layers \cite{bellec2017deep, chen20184096}. FPGA \cite{sheik2016synaptic} and hardware \cite{jerry2017ultra} implementations of spiking neuromorphic systems with synaptic stochasticity shows that it allows to increase the final accuracy of the networks while reducing memory requirements. On top of that, nanoelectronic devices with intrinsic cycle-to-cycle variability, such as memristors \cite{knag2014native} or VO 2 \cite{jerry2017ultra}, allow to reduce area and power consumption overhead of random number generation. Chen et al. \cite{chen20184096} also leverage probabilistic rewiring to increase their throughput. Fewer synapses imply fewer pulses to integrate, and the algorithmic timestep can thus be increased. Doing so, they report an acceleration of 8 times with an energy gain of 7.3 times for an accuracy loss of only 0.25\% on MNIST digit recognition, from 98.15\% to 97.9\%. Stochastic and quantized synapses can thus drastically reduce the memory requirement and power consumption of SNN accelerators, and even more with pruning insignificant weights.
The other approach consists of designing PEs that approximate their computation by employing modified algorithmic logic units \cite{han2013approximate}. Kim et al. \cite{kim2013energy} have shown that using carry skip adders enables speed and energy gains of 2.4 times and 43\%, respectively, when evaluating SNNs onto neuromorphic hardware for character recognition, with an accuracy loss of only 0.97\%.
Thus, approximate computing methods, both at the software and hardware levels, can enable important gains in power consumption and speed. However, as the complexity of the dataset increases, along with the depth of the network topology, such as using the ResNet \cite{he2016deep} on ImageNet \cite{russakovsky2015imagenet}, the accuracy loss becomes more important and may not be negligible anymore \cite{rastegari2016xnor}, especially for critical applications such as autonomous vehicles. It is thus unsure whether network compression techniques and approximate computing are scalable and applicable to any task.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%