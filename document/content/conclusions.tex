\section{Conclusions}
\label{sec:conclusions}
In this work, we accelerate SbS neural networks with a dot-product functional unit based on approximate computing combining the advantages of custom floating-point and logarithmic representations. This approach reduces computational latency, memory footprint, and power dissipation while preserving classification accuracy. For output quality monitoring, we applied noise tolerance plots as an intuitive visual measure to provide insights into the accuracy degradation of SbS networks under different approximate processing effects. This plot revels inherent error resilience, hence, reveals approximate processing possibilities.


We demonstrate our approach using a design exploration flow on a Xilinx Zynq-7020 with a deployment of SbS network for the MNIST classification task. This implementation achieves up to $20.5\times$ latency enhancement, $8\times$ weight memory footprint reduction, less than $0.5\%$ of accuracy degradation, with a $12.35\%$ of energy efficiency improvement over the standard floating-point hardware implementation. Furthermore, with a noise amplitude of $50\%$ added on top of the input images, the SbS network simulation presents an accuracy degradation of less than $5\%$. As output quality monitor, the resulting noise tolerance plots demonstrate a sufficient QoR for minimal impact on the overall accuracy of the neural network under the effects of the proposed approximation technique. These results suggest available room for further and more aggressive approximate processing approaches.


In summary, based on the relaxed need for fully accurate or deterministic computation of SbS neural networks, approximate computing techniques allow substantial enhancement in processing efficiency with moderated accuracy degradation.

\section * {Acknowledgments}\label{sec:Ack}
This work is funded by the \textit{Consejo Nacional de Ciencia y Tecnologia -- CONACYT} (the Mexican National Council for Science and Technology).
