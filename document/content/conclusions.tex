\section{Conclusions}
\label{sec:conclusions}

In this publication, we present a hardware design for approximate dot-product computation, with three design features: (1) the element-wise multiplication is done by adding integer exponents as well as accumulation is done by adding denormalized integer products, which increases computational throughput; (2) the synaptic weight vector uses either reduced custom floating-point or logarithmic representation, which reduces memory footprint; and (3) the neuron vector uses either standard or custom floating-point representation, which preserves inference accuracy.

This approach reduces computational latency, memory footprint, and power dissipation while preserving accuracy and noise robustness. Additionally, this solution does not require model retraining or quantization methods, representing a convenient alternative for emerging machine learning techniques. We demonstrate our approach with a design exploration flow on a Xilinx Zynq-7020 with a deployment of NMIST classification task, this implementation achieves $20.5\times$ latency enhancement, $8\times$ synaptic memory footprint reduction, and less than $0.5\%$ of accuracy degradation. Moreover, with positive additive uniformly distributed noise at $50\%$ of amplitude, the SbS network simulation presents an accuracy degradation of less than $1\%$ on the custom floating-point, and less than $5\%$ on the logarithmic computation, with 5-bit and 4-bit synaptic weight, respectably.

In conclusion, based on the relaxed need for fully precise or deterministic computation of SbS neural networks, approximate computing techniques allow substantial enhancement in processing efficiency with moderated accuracy degradation.

\section * {Acknowledgments}\label{sec:Ack}
This work is funded by the \textit{Consejo Nacional de Ciencia y Tecnologia -- CONACYT} (the Mexican National Council for Science and Technology).
