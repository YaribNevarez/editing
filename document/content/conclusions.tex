\section{Conclusions}
In this publication, we present a hardware architecture for non-quantized SbS network simulations based on optimized dot-product computation using hybrid custom floating-point and logarithmic number representation. To reduce latency and power dissipation, the proposed architecture performs element-wise multiplication by adding integer exponents, and denormalized product accumulation. To reduce memory footprint, the synaptic vector uses a reduced custom floating-point or logarithmic representation. To preserve accuracy and noise robustness, the neuron vector uses non-quantized standard floating-point representation.

The proposed hardware architecture is demonstrated on a Xilinx Zynq-7020 with a deployment of NMIST classification task. In this architecture, the SbS network computation using 5-bit custom floating-point and 4-bit logarithmic achieve 20.49x latency enhancement and an accuracy degradation of 0.33\% and 0.46\%, respectively. Moreover, in the case of adding 50\% amplitude of positive additive uniformly distributed noise to the input images, the SbS network simulation presents an accuracy degradation of less than 1\% on the custom floating-point, and less than 5\% on the logarithmic computation.

In conclusion, the proposed architecture reduces computational latency, memory footprint, and power dissipation while preserving accuracy and noise robustness of non-quantized SbS network models.

\section * {Acknowledgments}\label{sec:Ack}
This work is funded by the \textit{Consejo Nacional de Ciencia y Tecnologia -- CONACYT} (the Mexican National Council for Science and Technology).
