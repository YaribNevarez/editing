\section{Conclusions}
In this publication, we present a hardware architecture for non-quantized SbS network simulations based on optimized dot-product computation using custom floating-point and logarithmic number representation. To reduce latency and power dissipation, the proposed architecture performs element-wise multiplication by adding integer exponents, and denormalized product accumulation. To reduce memory footprint, the synaptic vector uses a precise custom floating-point or logarithmic representation. To preserve accuracy and noise robustness, the neuron vector uses non-quantized standard floating-point representation.

The proposed hardware architecture is demonstrated on a Xilinx Zynq-7020 with a deployment of NMIST classification task. In this architecture, the SbS network computation on hardware processing units using 5-bit custom floating-point and 4-bit logarithmic achieve 20.49x latency enhancement and accuracy of 98.97\% and 98.84\%, respectively. These results represent less than 0.5\% of accuracy degradation. Moreover, in the case of adding 50\% amplitude of positive additive uniformly distributed noise to the input images, the SbS network simulation presents an accuracy degradation of less than 1\% on the custom floating-point, and less than 5\% on the logarithmic computation.

In conclusion, the proposed dot-product architecture preserves accuracy and noise robustness while reducing latency, memory footprint, and power dissipation of non-quantized SbS network computation.

\section * {Acknowledgments}\label{sec:Ack}
This work is funded by the \textit{Consejo Nacional de Ciencia y Tecnologia -- CONACYT} (the Mexican National Council for Science and Technology).
