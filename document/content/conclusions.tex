\section{Conclusions}
\label{sec:conclusions}
In this publication, we accelerate SbS neural networks with a dot-product functional unit based on approximate computing.
This hardware module has the following three design features: (1) the pairwise product is done by adding exponents, then accumulation is done by adding denormalized products, which increases computational throughput; (2) the synaptic weight vector uses either reduced custom floating-point or logarithmic representation, which reduces memory footprint; and (3) the neuron vector uses either standard or custom floating-point representation, which preserves inference accuracy.

This approach reduces computational latency, memory footprint, and power dissipation while preserving accuracy and noise robustness. This solution is compatible with standard floating-point and, it does not require training algorithm adjustments, representing a convenient solution for efficient deployment of emerging ML algorithms on resource-limited devices.

We demonstrate our approach with a design exploration flow on a Xilinx Zynq-7020 with a deployment of NMIST classification task, this implementation achieves up to $20.49\times$ latency enhancement, $8\times$ synaptic memory footprint reduction, less than $0.5\%$ of accuracy degradation, and $12.35\%$ of energy efficiency improvement over the standard floating-point implementation. Moreover, with positive additive uniformly distributed noise at $50\%$ of amplitude on the input image, the SbS network simulation presents an accuracy degradation of less than $5\%$.

In conclusion, based on the relaxed need for fully accurate or deterministic computation of SbS neural networks, approximate computing techniques allow substantial enhancement in processing efficiency with moderated accuracy degradation.

\section * {Acknowledgments}\label{sec:Ack}
This work is funded by the \textit{Consejo Nacional de Ciencia y Tecnologia -- CONACYT} (the Mexican National Council for Science and Technology).
