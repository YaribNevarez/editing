
\section{Introduction}
\label{sec:introduction}
%%% General intro
\PARstart{A}{rtificial} Intelligence (AI) is increasingly attracting the interest of industry and academia; in particular,  Artificial Neural Networks (ANNs). Historically, ANNs can be classified into three different generations \cite{Design_Exploration_SbS_Trans20}: the first one is represented by the classical McCulloch and Pitts neuron model using discrete binary values as outputs; the second one is represented by more complex architectures as Multi-Layer Perceptrons and Convolutional Neural Networks (CNN) using continuous activation functions; while the third generation is represented by Spiking Neural Networks (SNNs) using spikes as means for information exchange between groups of neurons. Although the AI field is currently dominated by Deep Neural Networks (DNN) from the second generation, nowadays the SNNs belonging to the third generation are receiving considerable attention \cite{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20, SNN_Survey_Trans19} due to their advantages in terms of robustness and the
potential to achieve a power efficiency close to that of the human
brain (see section~\ref{sec:sbs} for more details).

%%% SbS intro
Among the family of SNNs, the Spike-by-Spike (SbS) neural network \cite{ernst2007efficient} is inspired by the natural computing of the mammalian brain, being a biologically plausible approach although with less complexity than other SNNs. The SbS model differs fundamentally from conventional ANNs since (a) the building block of the network are inference populations (IP) which are an optimized generative representation with non-negative values, (b) time progresses from one spike to the next, preserving the property of stochastically firing neurons, and (c) a network has only a small number of parameters, which is an advantageous stochastic version of Non-Negative Matrix Factorization (NNMF), which is noise-robust and easy to handle. In regard to biological realism and computational effort to simulate neural networks, these properties place the SbS network in between non-spiking NN and stochastically spiking NN \cite{rotermund2019Backpropagation}.

%%%%%%% Problem statement
In \cite{nevarez2020accelerator} we presented an accelerator framework which allows design exploration of dedicated hardware for SbS network computation in embedded systems. However, the high computational cost and memory footprint are the fundamental constrains of such resource-limited devices. Using logarithmic quantization have demonstrated higher classification accuracy than fixed-point at low resolutions; however, this alternative requires quantization-aware training methods that at the moment are inaccessible in SbS network models.

%%%%%%% Contributions
In this paper, we present a hardware architecture for non-quantized SbS network simulations based on approximate dot-product computation using hybrid custom floating-point and logarithmic number representation. The proposed hardware module for dot-product computation has the following main characteristics, (1) to increase computational throughput, the element-wise multiplication is done by adding integer exponents and accumulation is done by adding denormalized products, (2) to reduce memory footprint, the synaptic weight vector uses either reduced custom floating-point or logarithmic representation, and (3) to preserve inference accuracy, the neuron vector uses either standard or reduced custom floating-point representation.

We investigate the accuracy of the SbS network using standard 

We demonstrate our approach on a Xilinx Zynq-7020 with a deployment of NMIST classification task. The SbS network computation with synaptic weight vectors using 5-bit custom floating-point and 4-bit logarithmic representation achieve 20.49x latency enhancement and an accuracy degradation of 0.33\% and 0.46\%, respectively. Moreover, in the case of adding 50\% amplitude of positive additive uniformly distributed noise to the input images, the SbS network simulation presents an accuracy degradation of less than 1\% on the custom floating-point, and less than 5\% on the logarithmic computation.

In conclusion, the proposed architecture reduces computational latency, memory footprint, and power dissipation while preserving accuracy and noise robustness of non-quantized SbS network models.

In this paper, we present architectural optimizations based on the hybrid use of custom floating-point and logarithmic computation to reduce inference latency and memory footprint while preserving accuracy and noise robustness targeting non-quantized SbS network models.

The proposed hardware architecture exploits the available resources of the target platform: from pure embedded software on a single Central Processing Unit (CPU), scaling to a variable number of PUs for a low-cost or high-capacity FPGA. This design provides a configurable solution able to match different FPGA characteristics.

In this paper we present architectural optimizations using custom floating-point and logarithmic computation in a scalable design. We develop a hardware dot-product module for custom floating-point and logarithmic computation; this provides a good trade-off between resource utilization and throughput. These optimizations and methods are intended to be suitable for other ANNs, do not require model retraining, and can achieve comparable inference accuracy as the 32-bit floating-point counterpart.


To promote the research on SbS, the entire framework is made available to the public as an open-source project at http://www.ids.uni-bremen.de/sbs-framework.html

