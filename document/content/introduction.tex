
\section{Introduction}
\label{sec:introduction}
\PARstart{O}{ver} the past decade,  the exponential improvement in computing performance and the availability of large amounts of data are boosting the use of Artificial Intelligent (AI) in our daily lives. AI is increasingly attracting the interest of industry and academia; in particular,  Artificial Neural Networks (ANNs), an architecture inspired from the biological brain, is becoming the most frequently used form of AI. 

Historically, ANNs can be classified into three different generations \cite{Design_Exploration_SbS_Trans20}: The first one is represented by the classical McCulloch and Pitts neuron model; the second one is represented by more complex continuos-output architectures as Multi-Layer Perceptrons and Convolutional Neural Networks (CNN); while the third generation is represented by Spiking Neural Networks (SNNs). They differ fundamentally in the neural computation and the neural coding strategy: while the first generation uses discrete binary values as outputs, the second generation uses continuous activation functions where learning can be easily implemented. In contrast, the third generation, uses spikes as means for information exchange between groups of neurons. This strategy mimics how real neurons interact through short pulses (the so called action potentials).

Although the AI landscape is currently dominated by Deep Neural Networks (DNN) from the second generation, nowadays the SNNs belonging to the third generation are receiving considerable attention \cite{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20, SNN_Survey_Trans19} due to their advantages in terms of robustness and the
potential to achieve a power efficiency close to that of the human
brain (see section~\ref{sec:sbs} for more details). 

Among the family of SNNs, the Spike-by-Spike (SbS) neural network \cite{ernst2007efficient} is inspired by the natural computing of the mammalian brain, being a biologically plausible approach although with less complexity than other SNNs. The SbS model differs fundamentally from conventional artificial neural networks since (a) the building block of the network are inference populations (IP) which are an optimized generative representation with non-negative values, (b) time progresses from one spike to the next, preserving the property of stochastically firing neurons, and (c) a network has only a small number of parameters, which is an advantageous stochastic version of Non-Negative Matrix Factorization (NNMF), which is noise-robust and easy to handle. In regard to biological realism and computational effort to simulate neural networks, these properties place the SbS network in between non-spiking NN and stochastically spiking NN \cite{rotermund2019Backpropagation}. However, despite the favorable noise robustness and reduced complexity, the computational effort imposed by SbS is not suitable for applications in the current emerging technology of the Internet of Things (IoT) and Edge Computing.

To address the aforementioned problems, in \cite{nevarez2020accelerator} we presented a scalable hardware-software framework for SbS NN models targeting embedded systems applications. This framework deploys fully customizable SbS models allowing arbitrary dimensions, topologies, and acceleration configuration, ideal for experimentation and research on devices with limited resources, particularly in the field of IoT and Edge computing. However, the hardware architecture operates with standard floating-point computation having elevated inference latency, memory footprint.

In this paper, we present architectural optimizations based on the use of custom floating-point and logarithmic computation to reduce inference latency and memory footprint while preserving accuracy and noise robustness targeting non-quantized network models. Furthermore, the proposed hardware architecture consists of a collection of heterogeneous processing units (PU), and exploiting the principle of parallelization of SbS networks.

The proposed hardware architecture exploits the available resources of the target platform: from pure embedded software on a single Central Processing Unit (CPU), scaling to a variable number of PUs for a low-cost or high-capacity FPGA. This design provides a configurable solution able to match different FPGA characteristics.

In this paper we present architectural optimizations using custom floating-point and logarithmic computation in a scalable design. We develop a hardware dot-product module for custom floating-point and logarithmic computation; this provides a good trade-off between resource utilization and throughput. These optimizations and methods are intended to be suitable for other ANNs, do not require model retraining, and can achieve comparable inference accuracy as the 32-bit floating-point counterpart.


To promote the research on SbS, the entire framework is made available to the public as an open-source project at http://www.ids.uni-bremen.de/sbs-framework.html

