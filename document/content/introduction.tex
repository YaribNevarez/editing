
\section{Introduction}
\label{sec:introduction}
%%% General intro
\PARstart{A}{rtificial} Intelligence (AI) is increasingly attracting the interest of industry and academia; in particular,  Artificial Neural Networks (ANNs). Historically, ANNs can be classified into three different generations \cite{Design_Exploration_SbS_Trans20}: the first one is represented by the classical McCulloch and Pitts neuron model using discrete binary values as outputs; the second one is represented by more complex architectures as Multi-Layer Perceptrons and Convolutional Neural Networks (CNN) using continuous activation functions; while the third generation is represented by Spiking Neural Networks (SNNs) using spikes as means for information exchange between groups of neurons. Although the AI field is currently dominated by Deep Neural Networks (DNN) from the second generation, nowadays the SNNs belonging to the third generation are receiving considerable attention \cite{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20, SNN_Survey_Trans19} due to their advantages in terms of robustness and the
potential to achieve a power efficiency close to that of the human
brain (see section~\ref{sec:sbs} for more details).

%%% SbS intro
Among the family of SNNs, the SbS neural network \cite{ernst2007efficient} is inspired by the natural computing of the mammalian brain, being a biologically plausible approach although with less complexity than other SNNs. The SbS model differs fundamentally from conventional ANNs since (a) the building block of the network are inference populations (IP) which are an optimized generative representation with non-negative values, (b) time progresses from one spike to the next, preserving the property of stochastically firing neurons, and (c) a network has only a small number of parameters, which is an advantageous stochastic version of Non-Negative Matrix Factorization (NNMF), which is noise-robust and easy to handle. In regard to biological realism and computational effort to simulate neural networks, these properties place the SbS network in between non-spiking NN and stochastically spiking NN \cite{rotermund2019Backpropagation}.

%%%%%%% Problem statement
Although SbS networks provide numerous advantages over traditional ANNs and CNNs, their computational cost and memory footprint are the fundamental limitations for efficient deployment on resource-limited devices. As a newly emerging SNN algorithm, most SbS models use floating-point computation, which imposes a high cost on processing and memory footprint. Model quantization has the potential to improve computational performance on resource-limited devices; however, this solution is often accompanied by quantization-aware training methods that, in some cases, are problematic or even inaccessible, particularly in SNN algorithms\cite{zhang2018survey}.

%%%%%%% Contributions
In this paper, we present a hardware architecture for SbS network based on approximate dot-product computation using hybrid custom floating-point and logarithmic number representation. We use approximate computing as a design paradigm to leverage the intrinsic resilience of SbS networks to execute computations approximately, leading to higher efficiency and performance enhancement. This approach represents an alternative for efficient deployment of non-quantized SbS models on resource-limited devices. Our main contributions are as follows:

\begin{itemize}
	\item We develop a hardware module for approximate dot-product computation. The approximate dot-product consist of (1) the element-wise multiplication is done by adding integer exponents as well as accumulation is done by adding denormalized integer products, which increases computational throughput, (2) the synaptic weight vector uses either reduced custom floating-point or logarithmic representation, which reduces memory footprint, and (3) the neuron vector uses either standard or custom floating-point representation, which preserves inference accuracy.
	\item We address a design exploration evaluating computational latency, accuracy degradation, noise robustness, resource utilization and power dissipation. Experimental results show that the proposed architecture achieve 20.5x latency enhancement with less than 0.5\% of accuracy degradation.
	\item We present a noise robustness study using positive additive uniformly distributed noise on the input images. Experimental results show that the SbS network simulation presents an accuracy degradation of less than 1\% using custom floating-point, and less than 5\% using logarithmic computation.
	\item Our proposed hardware module for approximate dot-product computation is adaptable for other neural networks. This represents an alternative for efficient deployment of non-quantized floating-point neural network models on resource-limited devices.
\end{itemize}


The rest of the paper is organized as follows. Section~\ref{sec:related_work} covers the related work; Section~\ref{sec:background} introduces the background; Section~\ref{sec:system_design} describes the system design; Section~\ref{sec:experimental_results} presents the experimental results; Section~\ref{sec:conclusions} concludes the paper.


To promote the research on SbS, the entire framework is made available to the public as an open-source project at http://www.ids.uni-bremen.de/sbs-framework.html

