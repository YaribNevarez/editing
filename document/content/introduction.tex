
\section{Introduction}
\label{sec:introduction}
%%% General intro
\PARstart{A}{rtificial} Intelligence (AI) is increasingly attracting the interest of industry and academia; in particular,  Artificial Neural Networks (ANNs). Historically, ANNs can be classified into three different generations \cite{Design_Exploration_SbS_Trans20}: the first one is represented by the classical McCulloch and Pitts neuron model using discrete binary values as outputs; the second one is represented by more complex architectures as Multi-Layer Perceptrons and Convolutional Neural Networks (CNN) using continuous activation functions; while the third generation is represented by Spiking Neural Networks (SNNs) using spikes as means for information exchange between groups of neurons. Although the AI field is currently dominated by Deep Neural Networks (DNN) from the second generation, nowadays the SNNs belonging to the third generation are receiving considerable attention \cite{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20, SNN_Survey_Trans19} due to their advantages in terms of robustness and the
potential to achieve a power efficiency close to that of the human
brain (see section~\ref{sec:sbs} for more details).

%%% SbS intro
Among the family of SNNs, the SbS neural network \cite{ernst2007efficient} is inspired by the natural computing of the mammalian brain, being a biologically plausible approach although with less complexity than other SNNs. The SbS model differs fundamentally from conventional ANNs since (a) the building block of the network are inference populations (IP) which are an optimized generative representation with non-negative values, (b) time progresses from one spike to the next, preserving the property of stochastically firing neurons, and (c) a network has only a small number of parameters, which is an advantageous stochastic version of Non-Negative Matrix Factorization (NNMF), which is noise-robust and easy to handle. In regard to biological realism and computational effort to simulate neural networks, these properties place the SbS network in between non-spiking NN and stochastically spiking NN \cite{rotermund2019Backpropagation}.

%%%%%%% Problem statement
In \cite{nevarez2020accelerator} we presented an accelerator framework which allows design exploration of dedicated hardware for SbS network computation in embedded systems. However, the high computational cost and memory footprint are the fundamental constrains of such resource-limited devices. Using logarithmic quantization have demonstrated higher classification accuracy than fixed-point at low resolutions; however, this alternative requires quantization-aware training methods. As an alternative, we use approximate computing as a design paradigm to leverage the intrinsic resilience of SbS networks to execute computations approximately, leading to higher efficiency and performance enhancement.

%%%%%%% Contributions
In this paper, we present a hardware architecture for non-quantized SbS network simulations based on approximate dot-product computation using hybrid custom floating-point and logarithmic number representation. The proposed hardware module for dot-product computation has the following main characteristics, (1) to increase computational throughput, the element-wise multiplication is done by adding integer exponents as well as accumulation is done by adding denormalized integer products, (2) to reduce memory footprint, the synaptic weight vector uses either reduced custom floating-point or logarithmic representation, and (3) to preserve inference accuracy, the neuron vector uses either standard or reduced custom floating-point representation. On the proposed architecture, we evaluate computational latency, accuracy degradation, noise robustness, resource utilization and power dissipation.

The rest of the paper is organized as follows. Section~\ref{sec:related_work} covers the related work; Section~\ref{sec:background} introduces the background; Section~\ref{sec:system_design} describes the system design; Section~\ref{sec:experimental_results} presents the experimental results; Section~\ref{sec:conclusions} concludes the paper.


To promote the research on SbS, the entire framework is made available to the public as an open-source project at http://www.ids.uni-bremen.de/sbs-framework.html

