\title {Accelerating Spike-by-Spike Neural Networks with Approximate Dot-Product on FPGA}

\author{
	\uppercase{Yarib Nevarez}\authorrefmark{1},	
	\uppercase{David Rotermund}\authorrefmark{2},
	\uppercase{Klaus R. Pawelzik}\authorrefmark{3},
	\uppercase{Alberto Garcia-Ortiz}\authorrefmark{4} \IEEEmembership{Member, IEEE},
}

\address[1]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: nevarez@item.uni-bremen.de)}

\address[2]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: davrot@@neuro.uni-bremen.de)}

\address[3]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: pawelzik@@neuro.uni-bremen.de)}

\address[4]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: agaracia@item.uni-bremen.de)}

\tfootnote{This work is funded by the Consejo Nacional de Ciencia
	y Tecnologia - CONACYT (the Mexican National Council for
	Science and Technology)}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Yarib Nevarez (e-mail: nevarez@item.uni-bremen.de).}
.

\begin{abstract}
The Spike-by-Spike (SbS) neural network algorithm is a powerful machine learning (ML) technique for image classification with a remarkable noise robustness. However, deep SbS networks are highly compute and data intensive, requiring new approaches to improve deployment efficiency in resource limited-devices. In this article, we accelerate SbS neural networks with a dot-product hardware design based on approximate computing, this approach leverages the intrinsic error-tolerance of SbS neural networks. The proposed hardware architecture is demonstrated with a design exploration flow using high-level synthesis and a Xilinx FPGA. As a result, the proposed architecture achieves $20.5\times$ latency enhancement, $8\times$ synaptic memory footprint reduction, and less than $0.5\%$ of accuracy degradation on handwritten digit recognition task.
	
\end{abstract}

\begin{keywords}
Artificial intelligence, spiking neural networks, approximate computing, logarithmic, parameterisable floating-point, optimization, hardware accelerator, embedded systems, FPGA
\end{keywords}

\titlepgskip=-15pt

\maketitle
