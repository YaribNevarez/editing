\title {Accelerating Spike-by-Spike Neural Networks with Approximate Dot-Product on FPGA}

\author{
	\uppercase{Yarib Nevarez}\authorrefmark{1},	
	\uppercase{David Rotermund}\authorrefmark{2},
	\uppercase{Klaus R. Pawelzik}\authorrefmark{3},
	\uppercase{Alberto Garcia-Ortiz}\authorrefmark{4} \IEEEmembership{Member, IEEE},
}

\address[1]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: nevarez@item.uni-bremen.de)}

\address[2]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: davrot@@neuro.uni-bremen.de)}

\address[3]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: pawelzik@@neuro.uni-bremen.de)}

\address[4]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: agaracia@item.uni-bremen.de)}

\tfootnote{This work is funded by the Consejo Nacional de Ciencia
	y Tecnologia - CONACYT (the Mexican National Council for
	Science and Technology)}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Yarib Nevarez (e-mail: nevarez@item.uni-bremen.de).}
.

\begin{abstract}
The Spike-by-Spike (SbS) neural network algorithm is a powerful machine learning technique for image classification with a remarkable noise robustness. However, deep SbS networks are highly compute and data intensive, requiring new approaches to improve deployment efficiency in resource limited-devices. In this article, we present a hardware architecture for SbS network simulations based on approximate dot-product using hybrid custom floating-point and logarithmic computation, which leverages the extraordinary intrinsic error tolerance of SbS networks. The proposed hardware architecture is demonstrated with a design exploration using high-level synthesis and a Xilinx FPGA. As a result, the proposed architecture with synaptic representation using 5-bit custom floating-point, and 4-bit logarithmic achieve 20.5x latency enhancement with less than 0.5\% of accuracy degradation. Moreover, inserting positive additive uniformly distributed noise at 50\% amplitude to the input images, the SbS network simulation presents an accuracy degradation of less than 1\% using custom floating-point, and less than 5\% using logarithmic computation.
	
\end{abstract}

\begin{keywords}
Artificial intelligence, spiking neural networks, quantization, logarithmic computation, parameterisable floating-point, optimization, hardware accelerator, embedded systems, FPGA
\end{keywords}

\titlepgskip=-15pt

\maketitle
