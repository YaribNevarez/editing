\title {Spike-by-Spike Neural Networks using Custom Floating-Point and Logarithm Computation on FPGA}

\author{
	\uppercase{Yarib Nevarez}\authorrefmark{1},	
	\uppercase{David Rotermund}\authorrefmark{2},
	\uppercase{Klaus R. Pawelzik}\authorrefmark{3},
	\uppercase{Alberto Garcia-Ortiz}\authorrefmark{4} \IEEEmembership{Member, IEEE},
}

\address[1]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: nevarez@item.uni-bremen.de)}

\address[2]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: davrot@@neuro.uni-bremen.de)}

\address[3]{Institute for Theoretical Physics, University of Bremen, Bremen 28359, Germany (e-mail: pawelzik@@neuro.uni-bremen.de)}

\address[4]{Institute of Electrodynamics and Microelectronics, University of Bremen, Bremen 28359, Germany (e-mail: agaracia@item.uni-bremen.de)}

\tfootnote{This work is funded by the Consejo Nacional de Ciencia
	y Tecnologia - CONACYT (the Mexican National Council for
	Science and Technology)}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Yarib Nevarez (e-mail: nevarez@item.uni-bremen.de).}
.

\begin{abstract}
	Computational performance and memory footprint are recognized as the fundamental constraints in artificial neural networks (ANNs) for deployment in embedded systems. Implementations using logarithmic quantization have demonstrated huge potential to overcome the aforementioned limitations; however, this solution is often accompanied by quantization-aware training methods that, in some cases, are problematic or even inaccessible, particularly in emerging spiking neural network (SNN) algorithms. As an alternative, optimal hardware implementations may require the use of custom floating-point computation to improve performance while preserving accuracy of the neural network models.
	
	In this publication, we present a hardware architecture for non-quantized SbS network simulations based on an optimized dot-product using hybrid custom floating-point and logarithmic number representation.

	The proposed hardware architecture is demonstrated with a design exploration using high-level synthesis and a Xilinx FPGA. As a result, the proposed architecture with synaptic representation using 5-bit custom floating-point, and 4-bit logarithmic achieve 20.5x latency enhancement with less than 0.5\% of accuracy degradation. Moreover, inserting positive additive uniformly distributed noise at 50\% amplitude to the input images, the SbS network simulation presents an accuracy degradation of less than 1\% using custom floating-point, and less than 5\% using logarithmic computation.
	
\end{abstract}

\begin{keywords}
Artificial intelligence, spiking neural networks, quantization, logarithmic computation, parameterisable floating-point, optimization, hardware accelerator, embedded systems, FPGA
\end{keywords}

\titlepgskip=-15pt

\maketitle
