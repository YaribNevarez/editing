\relax 
\citation{schmidhuber2015deep,Taigman_2014_CVPR}
\citation{Design_Exploration_SbS_Trans20}
\citation{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20,SNN_Survey_Trans19}
\citation{ernst2007efficient}
\citation{rotermund2019Backpropagation}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:introduction}{{I}{1}}
\citation{lotrivc2012applicability,sarwar2016multiplier,mrazek2016design,du2014leveraging}
\citation{park2009dynamic,han2013approximate}
\citation{gupta2011impact,mittal2016survey}
\citation{venkataramani2015approximate}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Dot-product hardware module with (a) standard floating-point (IEEE 754) arithmetic, (b) hybrid custom floating-point approximation, and (c) hybrid logarithmic approximation.}}{2}}
\newlabel{fig:dot_product_unit}{{1}{2}}
\citation{han2013approximate}
\citation{bouvier2019spiking}
\citation{courbariaux2015binaryconnect,han2015deep,hubara2017quantized,rastegari2016xnor}
\citation{moons20160,whatmough201714}
\citation{rastegari2016xnor}
\citation{sun2018xnor}
\citation{lecun1989optimal,hassibi1992second,han2015learning}
\citation{molchanov2016pruning,li2016pruning,liu2018rethinking}
\citation{rathi2018stdp}
\citation{sen2017approximate}
\citation{srivastava2014dropout,wan2013regularization}
\citation{neftci2016stochastic,srinivasan2016magnetic}
\citation{buesing2011neural}
\citation{bellec2017deep,chen20184096}
\citation{sheik2016synaptic}
\citation{jerry2017ultra}
\citation{zhang2018survey}
\citation{han2013approximate}
\citation{kim2013energy}
\citation{he2016deep}
\citation{russakovsky2015imagenet}
\citation{rastegari2016xnor}
\citation{rotermund2018massively}
\citation{nevarez2020accelerator}
\citation{ernst2007efficient}
\citation{ernst2007efficient}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related work}{3}}
\newlabel{sec:related_work}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Approximate computing in neural networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}1}Network compression}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}2}Classical approximate computing}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Spike-by-Spike neural networks accelerators}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Background}{3}}
\newlabel{sec:background}{{III}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Spike-by-Spike Neural Networks}{3}}
\newlabel{sec:sbs}{{\unhbox \voidb@x \hbox {III-A}}{3}}
\citation{lecun1998mnist}
\citation{rotermund2019Backpropagation}
\citation{Rotermund500280}
\citation{nevarez2020accelerator}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) Performance classification of SbS NN versus equivalent CNN, and (b) example of the first pattern in the MNIST test data set with different amounts of noise.}}{4}}
\newlabel{fig:robustnes_sbs}{{2}{4}}
\newlabel{eq:sbs_update}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces SbS IPs as independent computational entities, (a) illustrates an input layer with a massive amount of IPs operating as independent computational entities, (b) shows a hidden layer with an arbitrary amount of IPs as independent computational entities, (c) exhibits a set of neurons grouped in an IP. }}{4}}
\newlabel{fig:SbS_layer}{{3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}System Design}{4}}
\newlabel{sec:system_design}{{IV}{4}}
\citation{rotermund2019Backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces System-level overview of the embedded software architecture.}}{5}}
\newlabel{fig:sw_stack}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Hardware architecture}{5}}
\newlabel{Hardware_architecture}{{\unhbox \voidb@x \hbox {IV-A}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Conv processing unit}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}1}Configuration mode}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces System-level hardware architecture with scalable number of heterogeneous PUs: {\it  Spike}, {\it  Conv}, {\it  Pool}, and {\it  FC}}}{5}}
\newlabel{fig:hw_sbs}{{5}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}2}Computation mode}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The {\it  Conv} processing unit and its six stages: (a) receive IP vector, (b) spike firing, (c) receive spike kernel, (d) update dynamics, (e) dispatch new IP vector, (f) dispatch output spike matrix.}}{6}}
\newlabel{fig:hw_conv}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}dot-product hardware module}{6}}
\newlabel{eq:dot_product}{{2}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}1}Dot-product with standard floating-point (IEEE 754) computation}{6}}
\newlabel{eq:dot_standard_float_latency}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dot-product hardware module with standard floating-point (IEEE 754) computation, (a) exhibits the initiation interval of 10 clock cycles, (b) presents the iteration latency of 19 clock cycles, (c) shows the pairwise product block in dark-gray, and (d) illustrates the accumulation block in light-gray.}}{7}}
\newlabel{fig:dot_product_float}{{7}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}2}Dot-product with hybrid custom floating-point and logarithmic approximation}{7}}
\newlabel{eq:dot_standard_custom_float_latency}{{4}{7}}
\newlabel{eq:dot_log_latency}{{5}{7}}
\citation{xilinx2015zynq}
\citation{nevarez2020accelerator}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dot-product hardware module with hybrid custom floating-point approximation, (a) exhibits the initiation interval of 2 clock cycles, (b) presents the iteration latency of 13 clock cycles, (c) shows the pairwise product blocks in dark-gray, and (d) illustrates the accumulation blocks in light-gray.}}{8}}
\newlabel{fig:dot_product_custom}{{8}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experimental results}{8}}
\newlabel{sec:experimental_results}{{V}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Dot-product hardware module with hybrid logarithmic approximation, (a) exhibits the initiation interval of 2 clock cycles, (b) presents the iteration latency of 9 clock cycles, (c) shows the pairwise product block in dark-gray, and (d) illustrates the accumulation blocks in light-gray.}}{8}}
\newlabel{fig:dot_product_log}{{9}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Performance benchmark}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}1}Benchmark on embedded CPU}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces SbS network structure for MNIST classification task. Input {\it  X}: Input layer with $28\times 28$ normalization modules for $28\times 28$ input pixel. From this layer spikes are send to layer {\it  H1}. {\it  H1}: Convolution layer {\it  H1} with $24\times 24$ IPs with $32$ neurons each. Every IP processes the spikes from $5\times 5$ spatial patches of the input pattern ($x$ and $y$ stride is $1$). {\it  H2}: $2\times 2$ pooling layer {\it  H2} ($x$ and $y$ stride is $2$) with $12\times 12$ IPs with $32$ neurons each. The weights between {\it  H1} and {\it  H2} are not learned but set to a fixed weight matrix that creates a competition between the {\it  32} features of {\it  H1}. {\it  H3}: $5\times 5$ convolution layer {\it  H3} ($x$ and $y$ stride is $1$) with $8\times 8$ IPs. Similar to {\it  H1} but with $64$ neuron for each IP. {\it  H4}: $2\times 2$ pooling layer {\it  H4} ($x$ and $y$ stride is $2$) with $4\times 4$ IPs with $64$ neurons each. This layer is similar to layer {\it  H2}. {\it  H5}: Fully connected layer {\it  H5}. $1,024$ neurons in one big IP which are fully connected to layer {\it  H4} and output layer {\it  HY}. {\it  HY}: Output layer {\it  HY} with $10$ neurons for the $10$ types of digits. selected.}}{9}}
\newlabel{fig:sbs_network}{{10}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation on embedded CPU.}}{9}}
\newlabel{tab:latency_sw}{{1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Computation on embedded CPU.}}{9}}
\newlabel{fig:latency_sw}{{11}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}2}Benchmark on processing units with standard floating-point (IEEE 754) computation}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces System overview of the top-level architecture with 8 processing units.}}{9}}
\newlabel{fig:hw_sbs_8_pu}{{12}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance of processing units with standard floating-point (IEEE 754) computation.}}{9}}
\newlabel{tab:latency_fp}{{2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Performance of processing units with standard floating-point (IEEE 754) computation.}}{9}}
\newlabel{fig:latency_pu_fp}{{13}{9}}
\citation{venkataramani2015approximate}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Performance bottleneck of cyclic computation on processing units with standard floating-point (IEEE 754) arithmetic, (a) exhibits the starting of $t_{PU}$ of {\it  Conv2} on a previous computation cycle, (b) presents $t_{CPU}$ of {\it  Conv2} on the current computation cycle, (c) shows the CPU waiting time (in gray color) for {\it  Conv2} as a busy resource (awaiting for {\it  Conv2} interruption), and (d) illustrates the $t_{f}$ from the previous computation cycle, the starting of $t_{PU}$ on the current computation cycle ({\it  Conv2} interruption on completion, and start current computation cycle).}}{10}}
\newlabel{fig:latency_pu_fp_cycle}{{14}{10}}
\newlabel{eq:time_cpu}{{6}{10}}
\newlabel{eq:time_pu}{{7}{10}}
\newlabel{eq:time_spike}{{8}{10}}
\newlabel{eq:time_finish}{{9}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Resource utilization and power dissipation of processing units with standard floating-point (IEEE 754) computation.}}{10}}
\newlabel{tab:resource_fp}{{3}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}3}Benchmark on noise tolerance plot}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Design exploration with hybrid custom floating-point and logarithmic approximation}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Noise tolerance on hardware PU with standard floating-point (IEEE 754) computation (benchmark/reference), (a) exhibits accuracy degradation applying $50\%$ of noise amplitude, and (b) illustrates convergence of inference with $400$ spikes.}}{11}}
\newlabel{fig:accuracy_vs_noise_pu_fp}{{15}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}1}Parameters for numeric representation of synaptic weight matrix}{11}}
\newlabel{sec:parameters}{{\unhbox \voidb@x \hbox {V-B}1}{11}}
\newlabel{eq:exp_max}{{10}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces $\qopname  \relax o{log}_2$-histogram of each synaptic weight matrix showing the percentage of matrix elements with given integer exponent.}}{11}}
\newlabel{fig:log2histogram}{{16}{11}}
\newlabel{eq:bits_exp}{{11}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}2}Design exploration for dot-product with hybrid custom floating-point approximation}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Resource utilization and power dissipation of processing units with hybrid custom floating-point approximation.}}{12}}
\newlabel{tab:resource_cfp}{{4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}3}Design exploration for dot-product whit hybrid logarithmic approximation}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performance of hardware processing units with hybrid custom floating-point approximation.}}{12}}
\newlabel{tab:latency_cfp}{{5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Performance on processing units with hybrid custom floating-point approximation, (a) exhibits computation schedule, (b) presents cyclic computation schedule, and (c) shows the performance of {\it  Conv2} from a previous computation cycle during the preprocessing of {\it  H1\_CONV} on the current computation cycle without bottleneck.}}{12}}
\newlabel{fig:latency_pu_cfp_cycle}{{17}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Results and discussion}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Noise tolerance on hardware PU with custom floating-point approximation, (a) exhibits accuracy degradation applying $50\%$ of noise amplitude, and (b) illustrates convergence of inference with $400$ spikes.}}{13}}
\newlabel{fig:accuracy_vs_noise_pu_cfp}{{18}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance of hardware processing units with hybrid logarithmic approximation.}}{13}}
\newlabel{tab:latency_log}{{6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Performance of processing units with hybrid logarithmic approximation, (a) exhibits computation schedule, and (b) illustrates cyclic computation schedule.}}{13}}
\newlabel{fig:latency_pu_log_cycle}{{19}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Resource utilization and power dissipation of processing units with hybrid logarithmic approximation.}}{13}}
\newlabel{tab:resource_log}{{7}{13}}
\bibstyle{IEEEtran}
\bibdata{../content/bibliography.bib}
\bibcite{schmidhuber2015deep}{1}
\bibcite{Taigman_2014_CVPR}{2}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Experimental results.}}{14}}
\newlabel{tab:results}{{8}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Noise tolerance on hardware PU with hybrid logarithmic approximation, (a) exhibits accuracy degradation applying $40\%$ of noise amplitude, (b) illustrates convergence of inference with $600$ spikes.}}{14}}
\newlabel{fig:accuracy_vs_noise_pu_log}{{20}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{14}}
\newlabel{sec:conclusions}{{VI}{14}}
\newlabel{sec:Ack}{{VI}{14}}
\@writefile{toc}{\contentsline {section}{REFERENCES}{14}}
\bibcite{Design_Exploration_SbS_Trans20}{3}
\bibcite{Spinnaker_Trans13}{4}
\bibcite{ernst2007efficient}{5}
\bibcite{SNN_Survey_Trans19}{6}
\bibcite{rotermund2019Backpropagation}{7}
\bibcite{lotrivc2012applicability}{8}
\bibcite{sarwar2016multiplier}{9}
\bibcite{mrazek2016design}{10}
\bibcite{du2014leveraging}{11}
\bibcite{park2009dynamic}{12}
\bibcite{han2013approximate}{13}
\bibcite{gupta2011impact}{14}
\bibcite{mittal2016survey}{15}
\bibcite{venkataramani2015approximate}{16}
\bibcite{bouvier2019spiking}{17}
\bibcite{courbariaux2015binaryconnect}{18}
\bibcite{han2015deep}{19}
\bibcite{hubara2017quantized}{20}
\bibcite{rastegari2016xnor}{21}
\bibcite{moons20160}{22}
\bibcite{whatmough201714}{23}
\bibcite{sun2018xnor}{24}
\bibcite{lecun1989optimal}{25}
\bibcite{hassibi1992second}{26}
\bibcite{han2015learning}{27}
\bibcite{molchanov2016pruning}{28}
\bibcite{li2016pruning}{29}
\bibcite{liu2018rethinking}{30}
\bibcite{rathi2018stdp}{31}
\bibcite{sen2017approximate}{32}
\bibcite{srivastava2014dropout}{33}
\bibcite{wan2013regularization}{34}
\bibcite{neftci2016stochastic}{35}
\bibcite{srinivasan2016magnetic}{36}
\bibcite{buesing2011neural}{37}
\bibcite{bellec2017deep}{38}
\bibcite{chen20184096}{39}
\bibcite{sheik2016synaptic}{40}
\bibcite{jerry2017ultra}{41}
\bibcite{zhang2018survey}{42}
\bibcite{kim2013energy}{43}
\bibcite{he2016deep}{44}
\bibcite{russakovsky2015imagenet}{45}
\bibcite{rotermund2018massively}{46}
\bibcite{nevarez2020accelerator}{47}
\bibcite{lecun1998mnist}{48}
\bibcite{Rotermund500280}{49}
\bibcite{xilinx2015zynq}{50}
\@writefile{toc}{\contentsline {subsection}{Yarib Nevarez}{16}}
\@writefile{toc}{\contentsline {subsection}{David Rotermund}{16}}
\@writefile{toc}{\contentsline {subsection}{Klaus R. Pawelzik}{16}}
\@writefile{toc}{\contentsline {subsection}{Alberto Garcia-Ortiz}{16}}
