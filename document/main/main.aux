\relax 
\citation{schmidhuber2015deep,Taigman_2014_CVPR}
\citation{Design_Exploration_SbS_Trans20}
\citation{Spinnaker_Trans13,ernst2007efficient,Design_Exploration_SbS_Trans20,SNN_Survey_Trans19}
\citation{mcdonnell2011benefits}
\citation{mcdonnell2011benefits}
\citation{mcdonnell2011benefits}
\citation{mcdonnell2011benefits}
\citation{mcdonnell2011benefits}
\citation{ernst2007efficient,Dapello2020.06.16.154542}
\citation{ernst2007efficient,Dapello2020.06.16.154542}
\citation{ernst2007efficient,Dapello2020.06.16.154542}
\citation{ernst2007efficient,Dapello2020.06.16.154542}
\citation{ernst2007efficient,Dapello2020.06.16.154542}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{davies2018loihi}
\citation{TrueNorth_Trans15}
\citation{TrueNorth_Trans15}
\citation{TrueNorth_Trans15}
\citation{TrueNorth_Trans15}
\citation{TrueNorth_Trans15}
\citation{Spinnaker_Trans13}
\citation{Spinnaker_Trans13}
\citation{Spinnaker_Trans13}
\citation{Spinnaker_Trans13}
\citation{Spinnaker_Trans13}
\citation{pfeiffer2018deep}
\citation{pfeiffer2018deep}
\citation{pfeiffer2018deep}
\citation{pfeiffer2018deep}
\citation{pfeiffer2018deep}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:introduction}{{I}{1}}
\citation{izhikevich2004model,amunts2019human}
\citation{izhikevich2004model,amunts2019human}
\citation{izhikevich2004model,amunts2019human}
\citation{izhikevich2004model,amunts2019human}
\citation{izhikevich2004model,amunts2019human}
\citation{rotermund2019Backpropagation,ernst2007efficient}
\citation{rotermund2019Backpropagation,ernst2007efficient}
\citation{rotermund2019Backpropagation,ernst2007efficient}
\citation{rotermund2019Backpropagation,ernst2007efficient}
\citation{rotermund2019Backpropagation,ernst2007efficient}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{rotermund2019Backpropagation}
\citation{roy2019towards,bouvier2019spiking,young2019review,TrueNorth_Trans15,Spinnaker_Trans13,davies2018loihi}
\citation{roy2019towards,bouvier2019spiking,young2019review,TrueNorth_Trans15,Spinnaker_Trans13,davies2018loihi}
\citation{roy2019towards,bouvier2019spiking,young2019review,TrueNorth_Trans15,Spinnaker_Trans13,davies2018loihi}
\citation{roy2019towards,bouvier2019spiking,young2019review,TrueNorth_Trans15,Spinnaker_Trans13,davies2018loihi}
\citation{roy2019towards,bouvier2019spiking,young2019review,TrueNorth_Trans15,Spinnaker_Trans13,davies2018loihi}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator,rotermund2018massively}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{ernst2007efficient,rotermund2019recurrentsbs}
\citation{ernst2007efficient,rotermund2019recurrentsbs}
\citation{ernst2007efficient,rotermund2019recurrentsbs}
\citation{ernst2007efficient,rotermund2019recurrentsbs}
\citation{ernst2007efficient,rotermund2019recurrentsbs}
\citation{zhang2018survey}
\citation{lotrivc2012applicability,sarwar2016multiplier,mrazek2016design,du2014leveraging}
\citation{park2009dynamic,han2013approximate}
\citation{gupta2011impact,mittal2016survey}
\citation{venkataramani2015approximate}
\citation{han2013approximate}
\citation{bouvier2019spiking}
\citation{courbariaux2015binaryconnect,han2015deep,hubara2017quantized,rastegari2016xnor}
\citation{moons20160,whatmough201714}
\citation{rastegari2016xnor}
\citation{sun2018xnor}
\citation{lecun1989optimal,hassibi1992second}
\citation{molchanov2016pruning,li2016pruning,liu2018rethinking}
\citation{rathi2018stdp}
\citation{sen2017approximate}
\citation{srivastava2014dropout,wan2013regularization}
\citation{neftci2016stochastic,srinivasan2016magnetic}
\citation{buesing2011neural}
\citation{bellec2017deep,chen20184096}
\citation{sheik2016synaptic}
\citation{jerry2017ultra}
\citation{zhang2018survey}
\citation{han2013approximate}
\citation{kim2013energy}
\citation{he2016deep}
\citation{russakovsky2015imagenet}
\citation{rastegari2016xnor}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Dot-product hardware module with (a) standard floating-point (IEEE 754) arithmetic, (b) hybrid custom floating-point approximation, and (c) hybrid logarithmic approximation.}}{3}}
\newlabel{fig:dot_product_unit}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related work}{3}}
\newlabel{sec:related_work}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Approximate computing in neural networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}1}Network compression}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}2}Classical approximate computing}{3}}
\citation{rotermund2018massively}
\citation{nevarez2020accelerator}
\citation{ernst2007efficient}
\citation{ernst2007efficient}
\citation{rotermund2019Backpropagation}
\citation{lecun1998mnist}
\citation{rotermund2019Backpropagation}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Spike-by-Spike neural networks accelerators}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Background}{4}}
\newlabel{sec:background}{{III}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Spike-by-Spike Neural Networks}{4}}
\newlabel{sec:sbs}{{\unhbox \voidb@x \hbox {III-A}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SbS IPs as independent computational entities, (a) illustrates an input layer with a massive amount of IPs operating as independent computational entities, (b) shows a hidden layer with an arbitrary amount of IPs as independent computational entities, (c) exhibits a set of neurons grouped in an IP. }}{4}}
\newlabel{fig:SbS_layer}{{2}{4}}
\newlabel{eq:sbs_update}{{1}{4}}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{rotermund2019Backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) Performance classification of SbS NN versus equivalent CNN, and (b) example of the first pattern in the MNIST test data set with different amounts of noise.}}{5}}
\newlabel{fig:robustnes_sbs}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}System Design}{5}}
\newlabel{sec:system_design}{{IV}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces System-level overview of the embedded software architecture.}}{5}}
\newlabel{fig:sw_stack}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Hardware architecture}{5}}
\newlabel{Hardware_architecture}{{\unhbox \voidb@x \hbox {IV-A}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Conv processing unit}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}1}Configuration mode}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces System-level hardware architecture with scalable number of heterogeneous PUs: {\it  Spike}, {\it  Conv}, {\it  Pool}, and {\it  FC}}}{6}}
\newlabel{fig:hw_sbs}{{5}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}2}Computation mode}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The {\it  Conv} processing unit and its six stages: (a) receive IP vector, (b) spike firing, (c) receive spike kernel, (d) update dynamics, (e) dispatch new IP vector, (f) dispatch output spike matrix.}}{6}}
\newlabel{fig:hw_conv}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}dot-product hardware module}{6}}
\newlabel{sec:dot-product_hardware_module}{{\unhbox \voidb@x \hbox {IV-C}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dot-product hardware module with (a) hybrid custom floating-point approximation, and (b) hybrid logarithmic approximation.}}{7}}
\newlabel{fig:product_unit_bitwidth}{{7}{7}}
\newlabel{eq:dot_product}{{2}{7}}
\newlabel{eq:exp_max}{{3}{7}}
\newlabel{eq:bits_exp}{{4}{7}}
\newlabel{eq:bits_bitwidth}{{5}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}1}Dot-product with standard floating-point computation}{7}}
\newlabel{eq:dot_standard_float_latency}{{6}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}2}Dot-product with hybrid custom floating-point and logarithmic approximation}{7}}
\citation{xilinx2015zynq}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dot-product hardware module with standard floating-point (IEEE 754) computation, (a) exhibits the initiation interval of 10 clock cycles, (b) presents the iteration latency of 19 clock cycles, (c) shows the pairwise product block in dark-gray, and (d) illustrates the accumulation block in light-gray.}}{8}}
\newlabel{fig:dot_product_float}{{8}{8}}
\newlabel{eq:dot_standard_custom_float_latency}{{7}{8}}
\newlabel{eq:dot_log_latency}{{8}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experimental results}{8}}
\newlabel{sec:experimental_results}{{V}{8}}
\citation{nevarez2020accelerator}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Dot-product hardware module with hybrid custom floating-point approximation, (a) exhibits the initiation interval of 2 clock cycles, (b) presents the iteration latency of 13 clock cycles, (c) shows the pairwise product blocks in dark-gray, and (d) illustrates the accumulation blocks in light-gray.}}{9}}
\newlabel{fig:dot_product_custom}{{9}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Dot-product hardware module with hybrid logarithmic approximation, (a) exhibits the initiation interval of 2 clock cycles, (b) presents the iteration latency of 9 clock cycles, (c) shows the pairwise product block in dark-gray, and (d) illustrates the accumulation blocks in light-gray.}}{9}}
\newlabel{fig:dot_product_log}{{10}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces SbS network structure for MNIST classification task. Input {\it  X}: Input layer with $28\times 28$ normalization modules for $28\times 28$ input pixel. From this layer spikes are send to layer {\it  H1}. {\it  H1}: Convolution layer {\it  H1} with $24\times 24$ IPs with $32$ neurons each. Every IP processes the spikes from $5\times 5$ spatial patches of the input pattern ($x$ and $y$ stride is $1$). {\it  H2}: $2\times 2$ pooling layer {\it  H2} ($x$ and $y$ stride is $2$) with $12\times 12$ IPs with $32$ neurons each. The weights between {\it  H1} and {\it  H2} are not learned but set to a fixed weight matrix that creates a competition between the {\it  32} features of {\it  H1}. {\it  H3}: $5\times 5$ convolution layer {\it  H3} ($x$ and $y$ stride is $1$) with $8\times 8$ IPs. Similar to {\it  H1} but with $64$ neuron for each IP. {\it  H4}: $2\times 2$ pooling layer {\it  H4} ($x$ and $y$ stride is $2$) with $4\times 4$ IPs with $64$ neurons each. This layer is similar to layer {\it  H2}. {\it  H5}: Fully connected layer {\it  H5}. $1,024$ neurons in one big IP which are fully connected to layer {\it  H4} and output layer {\it  HY}. {\it  HY}: Output layer {\it  HY} with $10$ neurons for the $10$ types of digits. selected.}}{9}}
\newlabel{fig:sbs_network}{{11}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Performance benchmark}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}1}Benchmark on embedded CPU}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation on embedded CPU.}}{10}}
\newlabel{tab:latency_sw}{{1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Computation on embedded CPU.}}{10}}
\newlabel{fig:latency_sw}{{12}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces System overview of the top-level architecture with 8 processing units.}}{10}}
\newlabel{fig:hw_sbs_8_pu}{{13}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}2}Benchmark on processing units with standard floating-point computation}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance of processing units with standard floating-point (IEEE 754) computation.}}{10}}
\newlabel{tab:latency_fp}{{2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Performance of processing units with standard floating-point (IEEE 754) computation.}}{10}}
\newlabel{fig:latency_pu_fp}{{14}{10}}
\newlabel{eq:time_cpu}{{9}{10}}
\newlabel{eq:time_pu}{{10}{10}}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{hrica2012floating}
\citation{venkataramani2015approximate}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Performance bottleneck of cyclic computation on processing units with standard floating-point (IEEE 754) arithmetic, (a) exhibits the starting of $t_{PU}$ of {\it  Conv2} on a previous computation cycle, (b) presents $t_{CPU}$ of {\it  Conv2} on the current computation cycle, (c) shows the CPU waiting time (in gray color) for {\it  Conv2} as a busy resource (awaiting for {\it  Conv2} interruption), and (d) illustrates the $t_{f}$ from the previous computation cycle, the starting of $t_{PU}$ on the current computation cycle ({\it  Conv2} interruption on completion, and start current computation cycle).}}{11}}
\newlabel{fig:latency_pu_fp_cycle}{{15}{11}}
\newlabel{eq:time_spike}{{11}{11}}
\newlabel{eq:time_finish}{{12}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Resource utilization and power dissipation of processing units with standard floating-point (IEEE 754) computation.}}{11}}
\newlabel{tab:resource_fp}{{3}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Resource utilization and power dissipation of multiplier and adder floating-point (IEEE 754) operator cores.}}{11}}
\newlabel{tab:LogiCORE}{{4}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-A}3}Benchmark on noise tolerance plot}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Noise tolerance on hardware PU with standard floating-point (IEEE 754) computation (benchmark/reference), (a) exhibits accuracy degradation applying $50\%$ of noise amplitude, and (b) illustrates convergence of inference with $400$ spikes.}}{12}}
\newlabel{fig:accuracy_vs_noise_pu_fp}{{16}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Design exploration with hybrid custom floating-point and logarithmic approximation}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces $\qopname  \relax o{log}_2$-histogram of each synaptic weight matrix showing the percentage of matrix elements with given integer exponent.}}{12}}
\newlabel{fig:log2histogram}{{17}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}1}Parameters for numeric representation of synaptic weight matrix}{12}}
\newlabel{sec:parameters}{{\unhbox \voidb@x \hbox {V-B}1}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}2}Design exploration for dot-product with hybrid custom floating-point approximation}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Resource utilization and power dissipation of processing units with hybrid custom floating-point approximation.}}{13}}
\newlabel{tab:resource_cfp}{{5}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-B}3}Design exploration for dot-product whit hybrid logarithmic approximation}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance of hardware processing units with hybrid custom floating-point approximation.}}{13}}
\newlabel{tab:latency_cfp}{{6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Performance on processing units with hybrid custom floating-point approximation, (a) exhibits computation schedule, (b) presents cyclic computation schedule, and (c) shows the performance of {\it  Conv2} from a previous computation cycle during the preprocessing of {\it  H1\_CONV} on the current computation cycle without bottleneck.}}{13}}
\newlabel{fig:latency_pu_cfp_cycle}{{18}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Noise tolerance on hardware PU with custom floating-point approximation, (a) exhibits accuracy degradation applying $50\%$ of noise amplitude, and (b) illustrates convergence of inference with $400$ spikes.}}{14}}
\newlabel{fig:accuracy_vs_noise_pu_cfp}{{19}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Performance of hardware processing units with hybrid logarithmic approximation.}}{14}}
\newlabel{tab:latency_log}{{7}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Results and discussion}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Performance of processing units with hybrid logarithmic approximation, (a) exhibits computation schedule, and (b) illustrates cyclic computation schedule.}}{14}}
\newlabel{fig:latency_pu_log_cycle}{{20}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Resource utilization and power dissipation of processing units with hybrid logarithmic approximation.}}{14}}
\newlabel{tab:resource_log}{{8}{14}}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\citation{nevarez2020accelerator}
\bibstyle{IEEEtran}
\bibdata{../content/bibliography.bib}
\bibcite{schmidhuber2015deep}{1}
\bibcite{Taigman_2014_CVPR}{2}
\bibcite{Design_Exploration_SbS_Trans20}{3}
\bibcite{Spinnaker_Trans13}{4}
\bibcite{ernst2007efficient}{5}
\bibcite{SNN_Survey_Trans19}{6}
\bibcite{mcdonnell2011benefits}{7}
\bibcite{Dapello2020.06.16.154542}{8}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Noise tolerance on hardware PU with hybrid logarithmic approximation, (a) exhibits accuracy degradation applying $40\%$ of noise amplitude, (b) illustrates convergence of inference with $600$ spikes.}}{15}}
\newlabel{fig:accuracy_vs_noise_pu_log}{{21}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{15}}
\newlabel{sec:conclusions}{{VI}{15}}
\newlabel{sec:Ack}{{VI}{15}}
\@writefile{toc}{\contentsline {section}{REFERENCES}{15}}
\bibcite{davies2018loihi}{9}
\bibcite{TrueNorth_Trans15}{10}
\bibcite{pfeiffer2018deep}{11}
\bibcite{izhikevich2004model}{12}
\bibcite{amunts2019human}{13}
\bibcite{rotermund2019Backpropagation}{14}
\bibcite{nevarez2020accelerator}{15}
\bibcite{rotermund2018massively}{16}
\bibcite{roy2019towards}{17}
\bibcite{bouvier2019spiking}{18}
\bibcite{young2019review}{19}
\bibcite{rotermund2019recurrentsbs}{20}
\bibcite{zhang2018survey}{21}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Experimental results.}}{16}}
\newlabel{tab:results}{{9}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Platform implementations.}}{16}}
\newlabel{tab:platform_comparison}{{10}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Power dissipation breakdown of platform implementations, (a) Ref. \cite  {nevarez2020accelerator} architecture with homogeneous AUs using standard floating-point arithmetic (IEEE 754), (b) reference architecture with specialized heterogeneous PUs using standard floating-point arithmetic (IEEE 754), (c) proposed architecture with hybrid custom floating-point approximation, and (d) proposed architecture with hybrid logarithmic approximation.}}{16}}
\newlabel{fig:platform_power_dissipation_breakdown}{{22}{16}}
\bibcite{lotrivc2012applicability}{22}
\bibcite{sarwar2016multiplier}{23}
\bibcite{mrazek2016design}{24}
\bibcite{du2014leveraging}{25}
\bibcite{park2009dynamic}{26}
\bibcite{han2013approximate}{27}
\bibcite{gupta2011impact}{28}
\bibcite{mittal2016survey}{29}
\bibcite{venkataramani2015approximate}{30}
\bibcite{courbariaux2015binaryconnect}{31}
\bibcite{han2015deep}{32}
\bibcite{hubara2017quantized}{33}
\bibcite{rastegari2016xnor}{34}
\bibcite{moons20160}{35}
\bibcite{whatmough201714}{36}
\bibcite{sun2018xnor}{37}
\bibcite{lecun1989optimal}{38}
\bibcite{hassibi1992second}{39}
\bibcite{molchanov2016pruning}{40}
\bibcite{li2016pruning}{41}
\bibcite{liu2018rethinking}{42}
\bibcite{rathi2018stdp}{43}
\bibcite{sen2017approximate}{44}
\bibcite{srivastava2014dropout}{45}
\bibcite{wan2013regularization}{46}
\bibcite{neftci2016stochastic}{47}
\bibcite{srinivasan2016magnetic}{48}
\bibcite{buesing2011neural}{49}
\bibcite{bellec2017deep}{50}
\bibcite{chen20184096}{51}
\bibcite{sheik2016synaptic}{52}
\bibcite{jerry2017ultra}{53}
\bibcite{kim2013energy}{54}
\bibcite{he2016deep}{55}
\bibcite{russakovsky2015imagenet}{56}
\bibcite{lecun1998mnist}{57}
\bibcite{xilinx2015zynq}{58}
\bibcite{hrica2012floating}{59}
\@writefile{toc}{\contentsline {subsection}{Yarib Nevarez}{17}}
\@writefile{toc}{\contentsline {subsection}{David Rotermund}{18}}
\@writefile{toc}{\contentsline {subsection}{Klaus R. Pawelzik}{18}}
\@writefile{toc}{\contentsline {subsection}{Alberto Garcia-Ortiz}{18}}
