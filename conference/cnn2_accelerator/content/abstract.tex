\begin{abstract}
In this article, we present a design exploration framework for floating-point convolutional neural networks (CNNs) acceleration on low-power, resource-limited embedded FPGAs targeting IoT sensor data analytic applications. We propose a scalable hardware architecture with customizable tensor processors (TPs) integrated with TensorFlow Lite. The implemented hardware optimization realizes hybrid custom floating-point and logarithmic dot-product approximation. This approach accelerates computation, reduces energy consumption and resource utilization while maintaining inference accuracy. Experimental results on MiniZed (XC7Z007S) and Zybo (XC7Z010) demonstrate peak acceleration and power efficiency of 105X and 5.5 GFLOP/s/W, respectively.
\end{abstract}

\begin{IEEEkeywords}
Artificial intelligence, convolutional neural networks, depthwise separable convolution, hardware accelerator, TensorFlow Lite, embedded systems, FPGA, custom floating-point, logarithmic computation, approximate computing
\end{IEEEkeywords}
