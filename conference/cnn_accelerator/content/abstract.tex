\begin{abstract}
Convolutional neural networks (CNNs) have become ubiquitous in the field of computer vision and image processing. Given the high computational demands of CNNs, dedicated hardware accelerators have been developed to improve performance and energy efficiency. However, most of commercial deep learning processing units (DPUs) are not targeting compatibility for resource-limited FPGAs.
In this publication, we present a dedicated hardware accelerator for TensorFlow (TF) Lite on embedded FPGA for convolution and depthwise convolution operators. The hardware design is implemented with high-level synthesis (HLS). This accelerator incorporates support for TF Lite quantization for fixed-point and floating-point representations. The proposed hardware optimization decomposes floating-point calculation for the convolution dot-product in order to accelerate computation, reduce energy consumption and resource utilization. To demonstrate the potential of the proposed architecture, we address a design exploration with custom-built CNNs with fixed-point quantization, floating-point single precision, half-precision, brain floating-point, NVidia's TensorFloat, and customized reduced formats for approximate computing including logarithmic representation. A single accelerator instance on a Xilinx Zynq-7020 achieves a peak runtime acceleration of $45\times$ on convolution operators compared to the embedded CPU, and $5\times$ compared with the standard Xilinx floating-point LogiCORE IP implementations. With regards to throughput and power efficiency, this accelerator at $150$ MHz yields $152$ MFLOP/s and $1.1$ TFLOPS/watt, respectively. The entire hardware design and the implemented TF Lite delegate extensions are available as open source project.
\end{abstract}

\begin{IEEEkeywords}
Artificial intelligence, convolutional neural networks, depthwise separable convolution, hardware accelerator, embedded systems, FPGA, custom floating-point, logarithmic, approximate computing
\end{IEEEkeywords}
