\begin{abstract}
Spiking neural networks (SNNs) represent a promising alternative to
conventional neural networks. In particular, the so-called
Spike-by-Spike (SbS) neural networks provide exceptional noise
robustness and reduced complexity. However, deep SbS networks
require a memory footprint and a computational cost unsuitable for
embedded applications. To address this problem, this work exploits
the intrinsic error resilience of neural networks to improve
performance and to reduce hardware complexity. More precisely,
we design a vector dot-product hardware unit based on approximate computing
with \replaced{configurable quality}{quality configurable design}
using hybrid custom floating-point and logarithmic number
representation. This approach reduces computational latency, memory
footprint, and power dissipation while preserving inference
accuracy. To demonstrate our approach, we address a design
exploration flow using high-level synthesis and a Xilinx SoC-FPGA.
\replaced{The}{As a result, the} proposed design \replaced{reduces
	$20.5\times$ computational latency and $8\times$ weight memory
	footprint, with}{ achieves $20.5\times$ latency enhancement,
	$8\times$ weight memory footprint reduction, and} less than
$0.5\%$ of accuracy degradation on \added{a} handwritten digit
recognition task.
	
\end{abstract}

\begin{IEEEkeywords}
Artificial intelligence, spiking neural networks, approximate computing, logarithmic, parameterisable floating-point, optimization, hardware accelerator, embedded systems, FPGA
\end{IEEEkeywords}
